{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["DFAPA8Jfolo2"],"authorship_tag":"ABX9TyNlGgFJ37iwGLfguHtD6cK7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"DFAPA8Jfolo2"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19385,"status":"ok","timestamp":1688662808429,"user":{"displayName":"Hasti Toossi","userId":"02154101839766812568"},"user_tz":240},"id":"EZfeDaSwwb38","outputId":"732da6a7-64ee-470c-ab98-71ac6b8c6cd8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","source":["%cd \"/content/gdrive/MyDrive/PerfPred/Experiment 1/lang2vec\"\n","!python3 setup.py install"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lRtKeScdp8NO","executionInfo":{"status":"ok","timestamp":1688662808900,"user_tz":240,"elapsed":475,"user":{"displayName":"Hasti Toossi","userId":"02154101839766812568"}},"outputId":"e4c383b5-2521-4f0e-f7e0-4dee6147b832"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/gdrive/MyDrive/PerfPred/Experiment 1/lang2vec'\n","/content\n","python3: can't open file '/content/setup.py': [Errno 2] No such file or directory\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"fUWBrJ8WuOXU","executionInfo":{"status":"error","timestamp":1688662809308,"user_tz":240,"elapsed":410,"user":{"displayName":"Hasti Toossi","userId":"02154101839766812568"}},"outputId":"5f8a9855-c820-497f-c0aa-fb2c1f85f822","colab":{"base_uri":"https://localhost:8080/","height":374}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-1ed2c43ee2b8>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgspread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlang2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang2vec\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ml2v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lang2vec'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from google.colab import auth\n","from google.auth import default\n","import gspread\n","import lang2vec.lang2vec as l2v\n","import numpy as np\n","import pandas as pd\n","import scipy\n","from pprint import pprint\n","from scipy.spatial.distance import cosine\n","from sklearn.metrics.pairwise import cosine_distances"]},{"cell_type":"code","source":["auth.authenticate_user()\n","creds, _ = default()\n","gc = gspread.authorize(creds)"],"metadata":{"id":"_uiQ8XQZvb7j","executionInfo":{"status":"aborted","timestamp":1688662809310,"user_tz":240,"elapsed":8,"user":{"displayName":"Hasti Toossi","userId":"02154101839766812568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Features"],"metadata":{"id":"6Q9sanZWeUXr"}},{"cell_type":"code","source":["main_features = ['syntax_average', 'phonology_average', 'inventory_average',\n","               'syntax_knn', 'phonology_knn', 'inventory_knn', 'fam', 'geo']\n","extra_features = [\"+\".join(['syntax_average', 'phonology_average', 'inventory_average']),\n","                  \"+\".join(['syntax_knn', 'phonology_knn', 'inventory_knn']),\n","                  \"+\".join(['syntax_average', 'phonology_average', 'inventory_average', 'fam', 'geo']),\n","                  \"+\".join(['syntax_knn', 'phonology_knn', 'inventory_knn', 'fam', 'geo']),\n","                  \"+\".join(main_features)]\n","all_features = main_features + extra_features\n","N = len(all_features)"],"metadata":{"id":"Iswf1DGsecde"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def feature_vecs(feats, langs, sheet):\n","  data = l2v.get_features(langs, '+'.join(feats), header=True)\n","  df = pd.DataFrame(data)\n","  worksheet = gc.open('l2v self-calculated distances').get_worksheet(sheet)\n","  worksheet.update_title('+'.join(feats)[:100])\n","  worksheet.update([df.columns.values.tolist()] + df.values.tolist())\n","  return df"],"metadata":{"id":"IPSf_X4fegQS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfs = []\n","langs = ['kan', 'guj', 'hin', 'sin', 'tam']\n","for i, feat in enumerate(all_features):\n","  dfs.append(feature_vecs([feat], [\"eng\"] + langs, i))"],"metadata":{"id":"omZkpUpAfE9V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def multiple_features(feats, lang, sheet):\n","  df = None\n","  for feat in feats:\n","    data = l2v.get_features(lang, feat, header=True)\n","    if df is None:\n","      df = pd.DataFrame(data, columns=[\"CODE\"])\n","    df[feat] = data[lang]\n","  worksheet = gc.open('l2v self-calculated distances').get_worksheet(sheet)\n","  worksheet.update_title(\"inv \" + lang)\n","  worksheet.update([df.columns.values.tolist()] + df.values.tolist())"],"metadata":{"id":"QjAR2nnyXYm2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inv_features = ['inventory_ethnologue', 'inventory_phoible_aa',\n","                'inventory_phoible_gm', 'inventory_phoible_saphon',\n","                'inventory_phoible_spa', 'inventory_phoible_ph',\n","                'inventory_phoible_ra', 'inventory_phoible_upsid']\n","# N + 7\n","multiple_features(inv_features + [\"|\".join(inv_features)], \"hi\", N + 7)\n","# data = l2v.get_features(\"hi\", inv_features, header=True)\n","# df = pd.DataFrame(data)\n","# worksheet = gc.open('l2v self-calculated distances').get_worksheet(N + 7)\n","# worksheet.update_title(\"inventory\")\n","# worksheet.update([df.columns.values.tolist()] + df.values.tolist())"],"metadata":{"id":"jf9XCeYRWVuP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ret = l2v.get_features([\"eng\", \"hin\"], \"|\".join(inv_features))\n","eng, hin = ret[\"eng\"], ret[\"hin\"]\n","print(np.arccos(np.dot(eng / np.linalg.norm(eng), hin / np.linalg.norm(hin))))\n","print(scipy.spatial.distance.cosine(eng, hin))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gb-Tr7kdYyM4","executionInfo":{"status":"ok","timestamp":1687207366720,"user_tz":240,"elapsed":2753,"user":{"displayName":"Hasti Toossi","userId":"02154101839766812568"}},"outputId":"c3ca6e60-4fa8-40c4-9d27-1234bdfd23fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.7357060106224771\n","0.25864287309665\n"]}]},{"cell_type":"markdown","source":["# Distances"],"metadata":{"id":"HWkWcjRMop2o"}},{"cell_type":"code","source":["def filter_df(df, langs):\n","  ret = df.copy()\n","  for lang in langs:\n","    ret = ret.loc[ret[lang] != '--']\n","  return ret"],"metadata":{"id":"9TUidg1trVsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def distances(feats, langs, sheet1, sheet2, val_per_lang):\n","  cols = [\"feature\"] + langs\n","  dist_df = pd.DataFrame(columns=cols)\n","  num_df = pd.DataFrame(columns=cols)\n","  for i, feat in enumerate(feats):\n","    df = dfs[all_features.index(feat)]\n","    row = {\"feature\": feat}\n","    num_row = {\"feature\": feat}\n","    all_df = filter_df(df, [\"eng\"] + langs)\n","    for lang in langs:\n","      if val_per_lang:\n","        val_df = filter_df(df, [\"eng\", lang])\n","      else:\n","        val_df = all_df\n","      num_row[lang] = len(val_df)\n","      if len(val_df) == 0 or not np.any(val_df[\"eng\"]) or not np.any(val_df[lang]):\n","        row[lang] = '--'\n","        continue\n","      val = scipy.spatial.distance.cosine(val_df[\"eng\"], val_df[lang])\n","      row[lang] = np.round(val, decimals=4)\n","    dist_df.loc[len(dist_df.index)] = row\n","    num_df.loc[len(num_df.index)] = num_row\n","  worksheet1 = gc.open('l2v self-calculated distances').get_worksheet(sheet1)\n","  worksheet1.update([dist_df.columns.values.tolist()] + dist_df.values.tolist())\n","  worksheet2 = gc.open('l2v self-calculated distances').get_worksheet(sheet2)\n","  worksheet2.update([num_df.columns.values.tolist()] + num_df.values.tolist())\n","  return dist_df"],"metadata":{"id":"MZDmiQxTjjns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["langs = ['kan', 'guj', 'hin', 'sin', 'tam']\n","dist_df = distances(all_features, langs, N, N + 1, True)"],"metadata":{"id":"FJ5MvPNIkhUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["langs = ['kan', 'guj', 'hin', 'sin', 'tam']\n","dist_df = distances(all_features, langs, N + 2, N + 3, False)"],"metadata":{"id":"zqw4j3fxrR1l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eng = dfs[5][\"eng\"]\n","kan = dfs[5][\"kan\"]\n","np.arccos(np.dot(eng / np.linalg.norm(eng), kan / np.linalg.norm(kan)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P5R3dMFB1w6f","executionInfo":{"status":"ok","timestamp":1687014235275,"user_tz":240,"elapsed":3,"user":{"displayName":"Hasti Toossi","userId":"02154101839766812568"}},"outputId":"454d4bab-a801-4698-cd9e-28126bc254ff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.744244995555758"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["dfff = feature_vecs([\"|\".join(['inventory_ethnologue', 'inventory_phoible_aa',\n","                              'inventory_phoible_gm', 'inventory_phoible_saphon',\n","                              'inventory_phoible_spa', 'inventory_phoible_ph',\n","                              'inventory_phoible_ra', 'inventory_phoible_upsid'])],\n","                    [\"eng\"] + langs, N + 7)"],"metadata":{"id":"zeaMgZIK3IoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["l2v.FEATURE_SETS"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bq--yyPi3MOt","executionInfo":{"status":"ok","timestamp":1687013890072,"user_tz":240,"elapsed":139,"user":{"displayName":"Hasti Toossi","userId":"02154101839766812568"}},"outputId":"34a96e6a-f7cc-4982-866f-950aadbe198b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['syntax_wals',\n"," 'phonology_wals',\n"," 'syntax_sswl',\n"," 'syntax_ethnologue',\n"," 'phonology_ethnologue',\n"," 'inventory_ethnologue',\n"," 'inventory_phoible_aa',\n"," 'inventory_phoible_gm',\n"," 'inventory_phoible_saphon',\n"," 'inventory_phoible_spa',\n"," 'inventory_phoible_ph',\n"," 'inventory_phoible_ra',\n"," 'inventory_phoible_upsid',\n"," 'syntax_knn',\n"," 'phonology_knn',\n"," 'inventory_knn',\n"," 'syntax_average',\n"," 'phonology_average',\n"," 'inventory_average',\n"," 'fam',\n"," 'id',\n"," 'geo',\n"," 'learned']"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["# Pre-calculated Distances"],"metadata":{"id":"Q5t-khGU7CTh"}},{"cell_type":"code","source":["def pre_calc_distances(feats, langs, sheet):\n","  cols=[\"distance\"] + langs\n","  dists = ['geographic', 'genetic', 'syntactic', 'phonological', 'inventory', 'featural']\n","  data = np.array(l2v.distance(dists, ['eng'] + langs))\n","  dist_df = pd.DataFrame(columns=cols)\n","  for i, dist in enumerate(dists):\n","    vals = data[i,0,1:]\n","    row = dict(zip(cols, [dist] + list(vals)))\n","    dist_df.loc[len(dist_df.index)] = row\n","  worksheet = gc.open('l2v self-calculated distances').get_worksheet(sheet)\n","  worksheet.update([dist_df.columns.values.tolist()] + dist_df.values.tolist())\n","  return dist_df"],"metadata":{"id":"VDF28IUd7FYb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["langs = ['kan', 'guj', 'hin', 'sin', 'tam']\n","dist_df = pre_calc_distances(all_features, langs, N + 5)"],"metadata":{"id":"yCCltkQr8IXy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Big Data Files"],"metadata":{"id":"luOtwLVRIPMg"}},{"cell_type":"code","source":["files = [\"FEATURAL\", \"GENETIC\", \"GEOGRAPHIC\", \"INVENTORY\", \"PHONOLOGICAL\", \"SYNTACTIC\"]\n","langs = ['kan', 'guj', 'hin', 'sin', 'tam']"],"metadata":{"id":"W74D2mDfK5Fm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for file in files:\n","  df = pd.read_csv(f\"data/distances/{file}.csv\", usecols=[\"G_CODE\", \"eng\"] + langs)\n","  df = df.loc[df[\"G_CODE\"].isin([\"eng\"] + langs)]\n","  df.to_csv(f\"data/truncated/{file}.csv\")"],"metadata":{"id":"EVoAvgbAIc2l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Main Sheet"],"metadata":{"id":"U7VT94Ro1Fbz"}},{"cell_type":"code","source":["def distance_with_eng_main(feats, langs, sheet):\n","  cols = [\"feature\"] + [lang + \"-eng\" for lang in langs]\n","  dist_df = pd.DataFrame(columns=cols)\n","  for i, feat in enumerate(feats):\n","    df = dfs[all_features.index(feat)]\n","    row = {\"feature\": feat}\n","    for lang in langs:\n","      val = scipy.spatial.distance.cosine(df[\"eng\"], df[lang])\n","      row[lang + \"-eng\"] = np.round(val, decimals=4)\n","    dist_df.loc[len(dist_df.index)] = row\n","  worksheet = gc.open('Experiment 1 Data').get_worksheet(sheet)\n","  worksheet.update([dist_df.columns.values.tolist()] + dist_df.values.tolist())\n","  return dist_df"],"metadata":{"id":"8PssT2UP1FDB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["langs = ['kan', 'guj', 'hin', 'sin', 'tam']\n","select_features = ['syntax_knn', 'phonology_knn', 'inventory_knn', 'fam', 'geo',\n","                  \"+\".join(['syntax_knn', 'phonology_knn', 'inventory_knn'])]\n","dist_df = distance_with_eng_main(select_features, langs, 3)\n","# not this anymore, geo & gen taken from pre-calculated"],"metadata":{"id":"jxMkmexg1Wzg"},"execution_count":null,"outputs":[]}]}
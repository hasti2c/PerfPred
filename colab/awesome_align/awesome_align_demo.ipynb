{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjc7LvIQbuMn"
      },
      "source": [
        "# AWESOME: Aligning Word Embedding Spaces of Multilingual Encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ipxcuO9vDgZ"
      },
      "source": [
        "[``awesome-align``](https://github.com/neulab/awesome-align) is a tool that can extract word alignments from multilingual BERT (mBERT) and allows you to fine-tune mBERT on parallel corpora for better alignment quality (see [our paper](https://arxiv.org/abs/2101.08231) for more details).\n",
        "\n",
        "This is a simple demo of how `awesome-align` extracts word alignments from mBERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJpRK-1_wQsJ"
      },
      "source": [
        "First, install and import the following packages. (Note that the original `awesome-align` tool does not require the `transformers` package.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODwJ_gQ8bnqR",
        "outputId": "1f57aac2-fd24-471f-d0c3-c8f4b1436241"
      },
      "source": [
        "!pip install transformers==3.1.0\n",
        "import torch\n",
        "import transformers\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.1.0 in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.1.95)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.8.1rc2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.1.0) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRvfawCbw2i7"
      },
      "source": [
        "Load the multilingual BERT model and its tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aPvLqT7eiry"
      },
      "source": [
        "model = transformers.BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RPxlavmxNmj"
      },
      "source": [
        "Input *tokenized* source and target sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfDM0w2kfHyJ"
      },
      "source": [
        "src = 'awesome-align is awesome !'\n",
        "tgt = '牛对齐 是 牛 ！'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpUa-ZqUxZ8Z"
      },
      "source": [
        "Run the model and print the resulting alignments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smW6s5JJflCN",
        "outputId": "75b5667e-b139-41c9-af35-b6c82bb74476"
      },
      "source": [
        "# pre-processing\n",
        "sent_src, sent_tgt = src.strip().split(), tgt.strip().split()\n",
        "token_src, token_tgt = [tokenizer.tokenize(word) for word in sent_src], [tokenizer.tokenize(word) for word in sent_tgt]\n",
        "wid_src, wid_tgt = [tokenizer.convert_tokens_to_ids(x) for x in token_src], [tokenizer.convert_tokens_to_ids(x) for x in token_tgt]\n",
        "ids_src, ids_tgt = tokenizer.prepare_for_model(list(itertools.chain(*wid_src)), return_tensors='pt', model_max_length=tokenizer.model_max_length, truncation=True)['input_ids'], tokenizer.prepare_for_model(list(itertools.chain(*wid_tgt)), return_tensors='pt', truncation=True, model_max_length=tokenizer.model_max_length)['input_ids']\n",
        "sub2word_map_src = []\n",
        "for i, word_list in enumerate(token_src):\n",
        "  sub2word_map_src += [i for x in word_list]\n",
        "sub2word_map_tgt = []\n",
        "for i, word_list in enumerate(token_tgt):\n",
        "  sub2word_map_tgt += [i for x in word_list]\n",
        "\n",
        "# alignment\n",
        "align_layer = 8\n",
        "threshold = 1e-3\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  out_src = model(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "  out_tgt = model(ids_tgt.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "\n",
        "  dot_prod = torch.matmul(out_src, out_tgt.transpose(-1, -2))\n",
        "\n",
        "  softmax_srctgt = torch.nn.Softmax(dim=-1)(dot_prod)\n",
        "  softmax_tgtsrc = torch.nn.Softmax(dim=-2)(dot_prod)\n",
        "\n",
        "  softmax_inter = (softmax_srctgt > threshold)*(softmax_tgtsrc > threshold)\n",
        "\n",
        "align_subwords = torch.nonzero(softmax_inter, as_tuple=False)\n",
        "align_words = set()\n",
        "for i, j in align_subwords:\n",
        "  align_words.add( (sub2word_map_src[i], sub2word_map_tgt[j]) )\n",
        "\n",
        "# printing\n",
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'\n",
        "\n",
        "for i, j in sorted(align_words):\n",
        "  print(f'{color.BOLD}{color.BLUE}{sent_src[i]}{color.END}==={color.BOLD}{color.RED}{sent_tgt[j]}{color.END}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[94mawesome-align\u001b[0m===\u001b[1m\u001b[91m牛对齐\u001b[0m\n",
            "\u001b[1m\u001b[94mis\u001b[0m===\u001b[1m\u001b[91m是\u001b[0m\n",
            "\u001b[1m\u001b[94mawesome\u001b[0m===\u001b[1m\u001b[91m牛\u001b[0m\n",
            "\u001b[1m\u001b[94m!\u001b[0m===\u001b[1m\u001b[91m！\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"Jnu_FaiCSs4v"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0hKX3PHhSf3B"},"outputs":[],"source":["from google.colab import auth\n","from google.auth import default\n","import gspread\n","import pandas as pd\n","import nltk\n","import math\n","import re\n","import typing as T\n","from nltk.corpus import stopwords\n","from itertools import product\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","auth.authenticate_user()\n","creds, _ = default()\n","gc = gspread.authorize(creds)"],"metadata":{"id":"yyvpeXueSyhb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fcc8064d-2602-46ea-a336-e27d56ccf935","executionInfo":{"status":"ok","timestamp":1685975930313,"user_tz":240,"elapsed":27879,"user":{"displayName":"Hasti Toossi","userId":"02154101839766812568"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# JS"],"metadata":{"id":"XBtx1ruBTefW"}},{"cell_type":"code","source":["# punctuation = [l.strip() for l in open('data/punctuation.txt').readlines()]\n","nltk.download('stopwords')\n","STOPWORDS = stopwords.words('english')\n","\n","# Add in punctuation, if desired\n","# stopwords += punctuation"],"metadata":{"id":"qw5JvaHxTR6Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685975930518,"user_tz":240,"elapsed":207,"user":{"displayName":"Hasti Toossi","userId":"02154101839766812568"}},"outputId":"c8744006-8b85-4229-9aed-a1ea755b218f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["def isNumber(num: str) -> bool:\n","    \"\"\"Check if a string is a number\n","    Args:\n","        num (str): piece of text\n","    Returns:\n","        bool: True iff the string can be converted\n","            to a number\n","    \"\"\"\n","    try:\n","        float(num)\n","        return True\n","    except ValueError:\n","        return False\n","\n","def computeFreqDistribution(doc: str, stopwords: bool = False) -> nltk.FreqDist:\n","    \"\"\"\n","    Computes the frequency of each word in a document\n","    Args:\n","        doc (str): string containing the entire document\n","        stopwords (bool): boolean flag indicating whether or not\n","            to remove the stopwords from the sentence. True indicates\n","            to remove the stopwords.\n","    Returns:\n","        nltk.FreqDist: frequency distribution\n","    \"\"\"\n","    tokens = nltk.regexp_tokenize(doc,'\\S+')\n","    filtered_tokens = [w.lower().strip('.,?!\"\\'') for w in tokens]\n","    consolidated_tokens = []\n","    for w in filtered_tokens:\n","        if isNumber(w):\n","            consolidated_tokens.append(\"<NUMBER>\")\n","            continue\n","        elif re.match(\"[\\d]+(pm|am)$\", w):\n","            consolidated_tokens.append(\"<TIME>\")\n","            continue\n","        elif re.match(\"[\\d]+:[\\d]+(pm|am)?$\", w):\n","            consolidated_tokens.append(\"<TIME>\")\n","            continue\n","        elif re.match(\"\\(?(\\w+)\\)?$\", w):\n","            m = re.match(\"\\(?(\\w+)\\)?$\", w)\n","            consolidated_tokens.append(m.group(1))\n","            continue\n","        else:\n","            consolidated_tokens.append(w)\n","\n","    # Remove stopwords from distribution\n","    if stopwords:\n","        consolidated_tokens = [w for w in consolidated_tokens if w not in STOPWORDS and w != \"\" ]\n","    else:\n","        consolidated_tokens = [w for w in consolidated_tokens if w != \"\"]\n","\n","    fd = nltk.FreqDist(consolidated_tokens)\n","    return fd\n","\n","def computeUnigramDistribution(doc: str, n_words: int = None, stopwords: bool = False) -> T.Tuple[dict, float]:\n","    \"\"\"\n","    Computes the relative frequencies (i.e., probs) of the most common unigrams\n","        in a document\n","    Args:\n","        doc (str): string containing the entire document\n","        n_words (int, optional): Number of most common words to consider.\n","            Defaults to None.\n","        stopwords: boolean flag indicating whether or not\n","            to remove the stopwords from the sentence. True indicates\n","            to remove the stopwords.\n","    Returns:\n","        dict: relative frequencies of the form dist[word] = prob\n","        float: sum of all the probabilities of the n_words most frequent unigrams\n","    \"\"\"\n","    fd = computeFreqDistribution(doc, stopwords)\n","    keys = list(fd.keys())[:n_words]\n","    values = list(fd.values())[:n_words]\n","    N = float(sum(values))\n","    dist = {}\n","    for key in keys:\n","        dist[key] = float(fd[key])/N\n","    return (dist,N)\n","\n","# Idea: Average prob of each word in both dist, include word if not present in dist1\n","def mergeDistributionJS(dist1: dict, dist2: dict) -> dict:\n","    \"\"\"\n","    Merges the two distributions used in the JS divergence\n","    Args:\n","        dist1 (dict): probability distribution of the form dist1[word] = prob\n","        dist2 (dict): probability distribution of the form dist2[word] = prob\n","    Returns:\n","        dict: New merged distribution including all words from both distributions\n","    \"\"\"\n","    mergeDist = {}\n","    for key in dist1.keys():\n","        mergeDist[key] = 1/2*dist1[key]\n","    for key in dist2.keys():\n","        if key in mergeDist.keys():\n","            mergeDist[key] += 1/2*dist2[key]\n","        else:\n","            mergeDist[key] = 1/2*dist2[key]\n","    return mergeDist\n","\n","# Idea: Compute sum of probability differences weighted by the log ratio\n","def KLDivergence(P: dict, M: dict, log_base: float = math.e) -> float:\n","    \"\"\"\n","    Computes the KL divergence for two distributions\n","        KL(P||M) = \\sum_{x \\in X}[p(x) * \\log(p(x)/q(x))]\n","    Args:\n","        P (dict): probability distribution of words\n","        M (dict): probability distribution of words\n","        log_base (float): Base value to use for log.\n","            Defaults to Euler's constant\n","    Returns:\n","        float: KL divergence of two distributions\n","    \"\"\"\n","    div = 0\n","    for key in P.keys():\n","        div += P[key] * math.log(P[key] / M[key], log_base)\n","    return div\n","\n","def JSDivergence(doc1: str, doc2: str, num_words: int = None, log_base: float = math.e, stopwords: bool = False) -> float:\n","    \"\"\"\n","    Calculates the JS Divergence value for two corpora\n","    Args:\n","        doc1 (str): string containing the entire document\n","        doc2 (str): string containing the entire document\n","        num_words (int): number of most frequent words to\n","            consider. Defaults to all words.\n","        log_base (float): Base value to use for log.\n","            Defaults to Euler's constant\n","        stopwords (bool): boolean flag indicating whether or not\n","            to remove the stopwords from the sentence. True indicates\n","            to remove the stopwords.\n","    Returns:\n","        float: the JS divergence of the two corpora\n","    \"\"\"\n","    P, N1 = computeUnigramDistribution(doc1, num_words, stopwords)\n","    Q, N2 = computeUnigramDistribution(doc2, num_words, stopwords)\n","    M = mergeDistributionJS(P, Q)\n","    js = 1/2*KLDivergence(P, M, log_base) + 1/2*KLDivergence(Q, M, log_base)\n","    return js / math.log(log_base)"],"metadata":{"id":"0p1zxItRYN4l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Single Domain"],"metadata":{"id":"iR8L1sHqpzle"}},{"cell_type":"markdown","source":["## Finding JSD"],"metadata":{"id":"0fY3_uTWp35W"}},{"cell_type":"code","source":["langs = ['gu', 'hi', 'ka', 'si', 'ta']\n","trains = ['cc_align', 'pmo/gov', 'bible']\n","tests = ['flores', 'bible', 'pmo/gov']\n","train_sizes = ['0k', '1k', '10k', '25k', '50k', '100k']\n","test_size = '1k'"],"metadata":{"id":"Bzu32wMip3Xu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lang_map = {\n","    'gu': 'Gujarati',\n","    'hi': 'Hindi',\n","    'ka': 'Kannada',\n","    'si': 'Sinhala',\n","    'ta': 'Tamil'\n","}\n","gov_map = {\n","    'gu': 'PrimeMinisterCorpus',\n","    'hi': 'PrimeMinisterCorpus',\n","    'ka': 'PrimeMinisterCorpus',\n","    'si': 'government',\n","    'ta': 'government'\n","}\n","bible_map = {\n","    'gu': 'new_bible_g1',\n","    'hi': 'new_bible_g1',\n","    'ka': 'new_bible_g1',\n","    'si': 'new_bible_g2',\n","    'ta': 'new_bible_g1'\n","}"],"metadata":{"id":"XMu8YcVnqbH4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_dataset_name(lang, dataset):\n","  match dataset:\n","    case 'cc_align':\n","      return 'cc_aligned'\n","    case 'bible':\n","      return bible_map[lang]\n","    case 'pmo/gov':\n","      return gov_map[lang]\n","    case 'flores':\n","      return 'flores'"],"metadata":{"id":"jeb4uffmqmdL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame(columns=['language', 'train set', 'train set size',\n","                           'test set', 'test set size', 'JSD'])\n","for lang, train, train_size, test in product(langs, trains, train_sizes, tests):\n","  if train_size == '0k':\n","    jsd = 1\n","  else:\n","    path = 'drive/MyDrive/PerfPred/Dataset/' + lang_map[lang] + '/'\n","    train_path = path + 'train/' + get_dataset_name(lang, train) + '/' + train_size + '/train-en_XX.txt'\n","    test_path = path + 'test/' + get_dataset_name(lang, test) + '/test-en_XX.txt'\n","    try:\n","      train_doc = open(train_path, 'r').read()\n","      test_doc = open(test_path, 'r').read()\n","      jsd = JSDivergence(train_doc, test_doc, stopwords = True)\n","    except FileNotFoundError:\n","      continue\n","\n","  row = {'language': lang,\n","        'train set': train,\n","        'train set size': train_size,\n","        'test set': test,\n","        'test set size': test_size,\n","        'JSD': jsd}\n","\n","  df.loc[len(df.index)] = row\n","  print(lang, train, train_size, test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3WKLG2fqoQ-","executionInfo":{"status":"ok","timestamp":1685994455883,"user_tz":240,"elapsed":423013,"user":{"displayName":"Hasti Toossi","userId":"02154101839766812568"}},"outputId":"77c0111a-edeb-40b7-861e-2288044fe604"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["gu cc_align 0k flores\n","gu cc_align 0k bible\n","gu cc_align 0k pmo/gov\n","gu cc_align 25k flores\n","gu cc_align 25k bible\n","gu cc_align 25k pmo/gov\n","gu cc_align 100k flores\n","gu cc_align 100k bible\n","gu cc_align 100k pmo/gov\n","gu pmo/gov 0k flores\n","gu pmo/gov 0k bible\n","gu pmo/gov 0k pmo/gov\n","gu pmo/gov 1k flores\n","gu pmo/gov 1k bible\n","gu pmo/gov 1k pmo/gov\n","gu pmo/gov 10k flores\n","gu pmo/gov 10k bible\n","gu pmo/gov 10k pmo/gov\n","gu pmo/gov 25k flores\n","gu pmo/gov 25k bible\n","gu pmo/gov 25k pmo/gov\n","gu bible 0k flores\n","gu bible 0k bible\n","gu bible 0k pmo/gov\n","gu bible 1k flores\n","gu bible 1k bible\n","gu bible 1k pmo/gov\n","gu bible 10k flores\n","gu bible 10k bible\n","gu bible 10k pmo/gov\n","gu bible 25k flores\n","gu bible 25k bible\n","gu bible 25k pmo/gov\n","hi cc_align 0k flores\n","hi cc_align 0k bible\n","hi cc_align 0k pmo/gov\n","hi cc_align 25k flores\n","hi cc_align 25k bible\n","hi cc_align 25k pmo/gov\n","hi cc_align 100k flores\n","hi cc_align 100k bible\n","hi cc_align 100k pmo/gov\n","hi pmo/gov 0k flores\n","hi pmo/gov 0k bible\n","hi pmo/gov 0k pmo/gov\n","hi pmo/gov 1k flores\n","hi pmo/gov 1k bible\n","hi pmo/gov 1k pmo/gov\n","hi pmo/gov 10k flores\n","hi pmo/gov 10k bible\n","hi pmo/gov 10k pmo/gov\n","hi pmo/gov 25k flores\n","hi pmo/gov 25k bible\n","hi pmo/gov 25k pmo/gov\n","hi pmo/gov 50k flores\n","hi pmo/gov 50k bible\n","hi pmo/gov 50k pmo/gov\n","hi bible 0k flores\n","hi bible 0k bible\n","hi bible 0k pmo/gov\n","hi bible 1k flores\n","hi bible 1k bible\n","hi bible 1k pmo/gov\n","hi bible 10k flores\n","hi bible 10k bible\n","hi bible 10k pmo/gov\n","hi bible 25k flores\n","hi bible 25k bible\n","hi bible 25k pmo/gov\n","ka cc_align 0k flores\n","ka cc_align 0k bible\n","ka cc_align 0k pmo/gov\n","ka cc_align 25k flores\n","ka cc_align 25k bible\n","ka cc_align 25k pmo/gov\n","ka cc_align 100k flores\n","ka cc_align 100k bible\n","ka cc_align 100k pmo/gov\n","ka pmo/gov 0k flores\n","ka pmo/gov 0k bible\n","ka pmo/gov 0k pmo/gov\n","ka pmo/gov 1k flores\n","ka pmo/gov 1k bible\n","ka pmo/gov 1k pmo/gov\n","ka pmo/gov 10k flores\n","ka pmo/gov 10k bible\n","ka pmo/gov 10k pmo/gov\n","ka pmo/gov 25k flores\n","ka pmo/gov 25k bible\n","ka pmo/gov 25k pmo/gov\n","ka bible 0k flores\n","ka bible 0k bible\n","ka bible 0k pmo/gov\n","ka bible 1k flores\n","ka bible 1k bible\n","ka bible 1k pmo/gov\n","ka bible 10k flores\n","ka bible 10k bible\n","ka bible 10k pmo/gov\n","ka bible 25k flores\n","ka bible 25k bible\n","ka bible 25k pmo/gov\n","si cc_align 0k flores\n","si cc_align 0k bible\n","si cc_align 0k pmo/gov\n","si cc_align 25k flores\n","si cc_align 25k bible\n","si cc_align 25k pmo/gov\n","si cc_align 100k flores\n","si cc_align 100k bible\n","si cc_align 100k pmo/gov\n","si pmo/gov 0k flores\n","si pmo/gov 0k bible\n","si pmo/gov 0k pmo/gov\n","si pmo/gov 1k flores\n","si pmo/gov 1k bible\n","si pmo/gov 1k pmo/gov\n","si pmo/gov 10k flores\n","si pmo/gov 10k bible\n","si pmo/gov 10k pmo/gov\n","si pmo/gov 25k flores\n","si pmo/gov 25k bible\n","si pmo/gov 25k pmo/gov\n","si pmo/gov 50k flores\n","si pmo/gov 50k bible\n","si pmo/gov 50k pmo/gov\n","si bible 0k flores\n","si bible 0k bible\n","si bible 0k pmo/gov\n","si bible 1k flores\n","si bible 1k bible\n","si bible 1k pmo/gov\n","si bible 10k flores\n","si bible 10k bible\n","si bible 10k pmo/gov\n","si bible 25k flores\n","si bible 25k bible\n","si bible 25k pmo/gov\n","ta cc_align 0k flores\n","ta cc_align 0k bible\n","ta cc_align 0k pmo/gov\n","ta cc_align 25k flores\n","ta cc_align 25k bible\n","ta cc_align 25k pmo/gov\n","ta cc_align 100k flores\n","ta cc_align 100k bible\n","ta cc_align 100k pmo/gov\n","ta pmo/gov 0k flores\n","ta pmo/gov 0k bible\n","ta pmo/gov 0k pmo/gov\n","ta pmo/gov 1k flores\n","ta pmo/gov 1k bible\n","ta pmo/gov 1k pmo/gov\n","ta pmo/gov 10k flores\n","ta pmo/gov 10k bible\n","ta pmo/gov 10k pmo/gov\n","ta pmo/gov 25k flores\n","ta pmo/gov 25k bible\n","ta pmo/gov 25k pmo/gov\n","ta pmo/gov 50k flores\n","ta pmo/gov 50k bible\n","ta pmo/gov 50k pmo/gov\n","ta bible 0k flores\n","ta bible 0k bible\n","ta bible 0k pmo/gov\n","ta bible 1k flores\n","ta bible 1k bible\n","ta bible 1k pmo/gov\n","ta bible 10k flores\n","ta bible 10k bible\n","ta bible 10k pmo/gov\n","ta bible 25k flores\n","ta bible 25k bible\n","ta bible 25k pmo/gov\n"]}]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"CdF6P5JoD29E","executionInfo":{"status":"ok","timestamp":1685994528336,"user_tz":240,"elapsed":130,"user":{"displayName":"Hasti Toossi","userId":"02154101839766812568"}},"outputId":"41ca5587-3f42-4400-ed24-59829535d906"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    language train set train set size test set test set size       JSD\n","0         gu  cc_align             0k   flores            1k  1.000000\n","1         gu  cc_align             0k    bible            1k  1.000000\n","2         gu  cc_align             0k  pmo/gov            1k  1.000000\n","3         gu  cc_align            25k   flores            1k  0.372864\n","4         gu  cc_align            25k    bible            1k  0.546171\n","..       ...       ...            ...      ...           ...       ...\n","169       ta     bible            10k    bible            1k  0.097009\n","170       ta     bible            10k  pmo/gov            1k  0.569990\n","171       ta     bible            25k   flores            1k  0.472301\n","172       ta     bible            25k    bible            1k  0.091791\n","173       ta     bible            25k  pmo/gov            1k  0.569598\n","\n","[174 rows x 6 columns]"],"text/html":["\n","  <div id=\"df-8c8a6d08-6cdb-4af2-9729-770a9803cdf9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>language</th>\n","      <th>train set</th>\n","      <th>train set size</th>\n","      <th>test set</th>\n","      <th>test set size</th>\n","      <th>JSD</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>gu</td>\n","      <td>cc_align</td>\n","      <td>0k</td>\n","      <td>flores</td>\n","      <td>1k</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>gu</td>\n","      <td>cc_align</td>\n","      <td>0k</td>\n","      <td>bible</td>\n","      <td>1k</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>gu</td>\n","      <td>cc_align</td>\n","      <td>0k</td>\n","      <td>pmo/gov</td>\n","      <td>1k</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>gu</td>\n","      <td>cc_align</td>\n","      <td>25k</td>\n","      <td>flores</td>\n","      <td>1k</td>\n","      <td>0.372864</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>gu</td>\n","      <td>cc_align</td>\n","      <td>25k</td>\n","      <td>bible</td>\n","      <td>1k</td>\n","      <td>0.546171</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>169</th>\n","      <td>ta</td>\n","      <td>bible</td>\n","      <td>10k</td>\n","      <td>bible</td>\n","      <td>1k</td>\n","      <td>0.097009</td>\n","    </tr>\n","    <tr>\n","      <th>170</th>\n","      <td>ta</td>\n","      <td>bible</td>\n","      <td>10k</td>\n","      <td>pmo/gov</td>\n","      <td>1k</td>\n","      <td>0.569990</td>\n","    </tr>\n","    <tr>\n","      <th>171</th>\n","      <td>ta</td>\n","      <td>bible</td>\n","      <td>25k</td>\n","      <td>flores</td>\n","      <td>1k</td>\n","      <td>0.472301</td>\n","    </tr>\n","    <tr>\n","      <th>172</th>\n","      <td>ta</td>\n","      <td>bible</td>\n","      <td>25k</td>\n","      <td>bible</td>\n","      <td>1k</td>\n","      <td>0.091791</td>\n","    </tr>\n","    <tr>\n","      <th>173</th>\n","      <td>ta</td>\n","      <td>bible</td>\n","      <td>25k</td>\n","      <td>pmo/gov</td>\n","      <td>1k</td>\n","      <td>0.569598</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>174 rows × 6 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c8a6d08-6cdb-4af2-9729-770a9803cdf9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8c8a6d08-6cdb-4af2-9729-770a9803cdf9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8c8a6d08-6cdb-4af2-9729-770a9803cdf9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":157}]},{"cell_type":"code","source":["gsheet_df = pd.DataFrame(columns=['train set', 'train set size', 'test set',\n","                                  'test set size', 'ka', 'gu', 'hi', 'si', 'ta'])\n","for train, train_size, test in product(trains, train_sizes, tests):\n","  row = {\n","      'train set': train,\n","      'train set size': train_size,\n","      'test set': test,\n","      'test set size': test_size\n","  }\n","  for lang in langs:\n","    slice = df[(df['train set'] == train) & (df['train set size'] == train_size) &\n","               (df['test set'] == test) & (df['test set size'] == test_size) &\n","               (df['language'] == lang)]\n","    if len(slice) == 1:\n","      row[lang] = slice.at[slice.index[0], 'JSD']\n","    else:\n","      row[lang] = -1\n","  if all([row[lang] == -1 for lang in langs]):\n","    continue\n","  gsheet_df.loc[len(gsheet_df.index)] = row"],"metadata":{"id":"_o3Jvbu1C9FJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["worksheet = gc.open('Experiment 1 Tab').get_worksheet(1)\n","worksheet.update([gsheet_df.columns.values.tolist()] + gsheet_df.values.tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0YSdP-NGDPjy","executionInfo":{"status":"ok","timestamp":1685995114417,"user_tz":240,"elapsed":918,"user":{"displayName":"Hasti Toossi","userId":"02154101839766812568"}},"outputId":"c408d688-dabf-40c9-e2e1-3e3f8500f2f4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'spreadsheetId': '1V0fqzJOUg62M1RGGmd2LtKv0ZA76yYkY8Xo0iYaHr5w',\n"," 'updatedRange': \"'JSD-Single-Dom'!A1:I37\",\n"," 'updatedRows': 37,\n"," 'updatedColumns': 9,\n"," 'updatedCells': 333}"]},"metadata":{},"execution_count":162}]},{"cell_type":"markdown","source":["# Mixed Domain"],"metadata":{"id":"u598ndi2pp37"}},{"cell_type":"markdown","source":["## Finding JSD"],"metadata":{"id":"NZZXRiOiBZ95"}},{"cell_type":"code","source":["langs = ['gu', 'hi', 'ka', 'si', 'ta']\n","trains = [('cc_align', 'bible'), ('cc_align', 'pmo/gov'), ('bible', 'pmo/gov'),\n","          ('cc_align', 'pmo/gov', 'bible')]\n","tests = ['pmo/gov', 'bible', 'flores']\n","train_size = '25k'\n","test_size = '1k'"],"metadata":{"id":"0y8IGTMNI7Ir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lang_map = {\n","    'gu': 'Gujarati',\n","    'hi': 'Hindi',\n","    'ka': 'Kannada',\n","    'si': 'Sinhala',\n","    'ta': 'Tamil'\n","}\n","gov_map = {\n","    'gu': 'PrimeMinisterCorpus',\n","    'hi': 'PrimeMinisterCorpus',\n","    'ka': 'PrimeMinisterCorpus',\n","    'si': 'government',\n","    'ta': 'government'\n","}\n","bible_map = {\n","    'gu': 'new_bible_g1',\n","    'hi': 'new_bible_g1',\n","    'ka': 'new_bible_g1',\n","    'si': 'new_bible_g2',\n","    'ta': 'new_bible_g1'\n","}"],"metadata":{"id":"yY_nl9UgMi9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_dataset_name(lang, dataset):\n","  match dataset:\n","    case 'cc_align':\n","      return 'cc_aligned'\n","    case 'bible':\n","      return bible_map[lang]\n","    case 'pmo/gov':\n","      return gov_map[lang]\n","    case 'flores':\n","      return 'flores'"],"metadata":{"id":"PccDrEHGPsss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame(columns=['language', 'train set', 'train set size',\n","                           'test set', 'test set size', 'JSD'])\n","for lang, train, test in product(langs, trains, tests):\n","  path = 'drive/MyDrive/PerfPred/Dataset/' + lang_map[lang] + '/'\n","  train_name = '+'.join([get_dataset_name(lang, data) for data in train])\n","  train_size_name = '+'.join([train_size] * len(train))\n","  train_path = path + 'train/mixed/' + train_name + '/' + train_size_name + '/train-en_XX.txt'\n","  test_path = path + 'test/' + get_dataset_name(lang, test) + '/test-en_XX.txt'\n","  try:\n","    train_doc = open(train_path, 'r').read()\n","    test_doc = open(test_path, 'r').read()\n","    jsd = JSDivergence(train_doc, test_doc, stopwords = True)\n","  except FileNotFoundError:\n","    jsd = -1\n","\n","  row = {'language': lang,\n","         'train set': train,\n","         'train set size': train_size,\n","         'test set': test,\n","         'test set size': test_size,\n","         'JSD': jsd}\n","\n","  df.loc[len(df.index)] = row"],"metadata":{"id":"GIXilcblMrbo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Writing to GSheet"],"metadata":{"id":"6vCq_AEfgnEl"}},{"cell_type":"code","source":["auth.authenticate_user()\n","creds, _ = default()\n","gc = gspread.authorize(creds)"],"metadata":{"id":"cbS9ZX-KbWBC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gsheet_df = pd.DataFrame(columns=['train set', 'train set size', 'test set',\n","                                  'test set size', 'ka', 'gu', 'hi', 'si', 'ta'])\n","for train, test in product(trains, tests):\n","  row = {\n","      'train set': '+'.join(train),\n","      'train set size': '+'.join([train_size] * len(train)),\n","      'test set': test,\n","      'test set size': test_size\n","  }\n","  for lang in langs:\n","    slice = df[(df['train set'] == train) & (df['train set size'] == train_size) &\n","               (df['test set'] == test) & (df['test set size'] == test_size) &\n","               (df['language'] == lang)]\n","    if len(slice) == 1:\n","      row[lang] = slice.at[slice.index[0], 'JSD']\n","    else:\n","      row[lang] = -1\n","      pass\n","  gsheet_df.loc[len(gsheet_df.index)] = row"],"metadata":{"id":"UryuncHTkl1s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["worksheet = gc.open('Experiment 1 Data').get_worksheet(0)\n","worksheet.update([gsheet_df.columns.values.tolist()] + gsheet_df.values.tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nDAPELkNgpCx","executionInfo":{"status":"ok","timestamp":1685992738538,"user_tz":240,"elapsed":1086,"user":{"displayName":"Hasti Toossi","userId":"02154101839766812568"}},"outputId":"0476b74a-210e-4ec0-feff-044be5045099"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'spreadsheetId': '1awrtFTqiNVT4hbbQlD2ZLB5mPj0BqkCAE4RgahtrtgQ',\n"," 'updatedRange': \"'Mixed Domain'!A1:I13\",\n"," 'updatedRows': 13,\n"," 'updatedColumns': 9,\n"," 'updatedCells': 117}"]},"metadata":{},"execution_count":143}]}]}